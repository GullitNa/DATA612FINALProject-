{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DATA 612 Final Project Planning Document**\n",
        "\n",
        "**Gullit Navarrete**\n",
        "\n",
        "**7/13/25**"
      ],
      "metadata": {
        "id": "UAMVwOJTbCo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "As an overview of what I intend for my final project presentation, I'm reusing my previous project 4's dataset MovieLens but instead of using the previous 100K size, I'm expanding to the 1M dataset. The project will be a hybrid recommender system that is suppose to add emphasis to the \"tag enchanced\" collaborative filtering system using Spark’s ALS latent factors with item features drawn from the Tag Genome.\n",
        "\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Reusing the previous dataset that I've worked on from last project. Except that this time I'm expanding from the MovieLens 100K Movie Ratings to the 1M Movie Ratings, I'll code the download and the unpacking of MovieLen's zip file for replication below."
      ],
      "metadata": {
        "id": "orA5sdhynZpa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd0BtwGl_OCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba25452-d8c7-4da5-ec34-566cfab90d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ml-100k/allbut.pl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Download\n",
        "!wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
        "\n",
        "# Unpack\n",
        "!unzip -q ml-1m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll be using Tag Genome afterwards through the project for joining the Tag Genome to MovieLens, every movie gains specific features that augment the latent ALS factors which allow the system to surface recommendations based both on collaborative signals and shared content descriptors.\n",
        "\n",
        "**Tools**\n",
        "*   Google Colab\n",
        "*   Spark MLlib\n",
        "*   PySpark\n",
        "*   TruncatedSVD\n",
        "\n",
        "**Algorithms**\n",
        "\n",
        "This Tag Enhanced Collaborative Filtering would require data ingestion and preprocessing in PySpark first. By loading the MovieLens 1M ratings into a Spark DataFrame and pivot the Tag Genome scores into a movie×tag matrix. Once both tables are in memory, we train a standard ALS model in MLlib to learn low-dimensional user and item embeddings purely from collaborative signals. As well as reducing each movie’s 1100 dimensional tag vector using Spark (or TruncatedSVD) to a more compact and concise 20 dimensional representation. Afterwards, I plan to (possible but will be revisted) fuse these two feature sets (which are ALS latent factors and tag PCA embeddings) into a single hybrid predictor. Depending on the result, that fusion happens either in Spark with Factorization Machines (which jointly model both latent and explicit features) or in Python using a small neural regressor. We then evaluate both RMSE on held‐out ratings and “beyond accuracy” metrics (diversity and serendipity) to measure the benefit of adding tag content.\n",
        "\n",
        "**Scalability**\n",
        "\n",
        "I'm fully aware that Google colab/python can handle the movielens 1M dataset and tag PCA locally, but will struggle when scaling to tens of millions of ratings or processing high-dimensional side info. When the ratings volume exceeds more than what fits in memory, or when you need frequent retraining or hyperparameter tuning on large combined feature sets, Spark’s distributed data shuffling, and in-memory ALS are important for maintenance and performance."
      ],
      "metadata": {
        "id": "wXun5s2eh8XL"
      }
    }
  ]
}